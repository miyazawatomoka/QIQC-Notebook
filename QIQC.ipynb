{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "import nltk\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = torchtext.vocab.Vectors(\"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIX_LENGTH = 70\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "TEXT = torchtext.data.Field(tokenize=torchtext.data.get_tokenizer('toktok'), init_token='<SOS>', \n",
    "                            eos_token='<EOS>',lower=True, fix_length=FIX_LENGTH, stop_words=None, \n",
    "                            batch_first=True)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True, batch_first=True, \n",
    "                             dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchtext.data.TabularDataset(\n",
    "    path='../input/train.csv', format = 'csv', \n",
    "    fields=[('qid', None),\n",
    "            ('question_text', TEXT), \n",
    "            ('target', LABEL)], \n",
    "    skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_data.split(split_ratio=0.95, strata_field='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_iter = torchtext.data.Iterator(dataset=train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_iter = torchtext.data.Iterator(dataset=test, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors = glove_vectors, max_size=20000, min_freq=10)\n",
    "vocab = TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 20004\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained embeddings shape: torch.Size([20004, 300])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = vocab.vectors\n",
    "print(f\"Pretrained embeddings shape: {pretrained_embeddings.shape}\")\n",
    "EMBEDDING_LENGTH = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_DROPOUT = 0.1\n",
    "CNN_KERNEL_NUM = 50\n",
    "class TextCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_weight, is_static=False):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.with_embedding = False\n",
    "        in_channel = 1\n",
    "        out_channel = CNN_KERNEL_NUM\n",
    "        kernel_sizes = [3, 4, 5]\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_weight, freeze=(not is_static))\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(in_channel, out_channel, (K, EMBEDDING_LENGTH)) for K in kernel_sizes])\n",
    "\n",
    "        self.dropout = nn.Dropout(CNN_DROPOUT)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * out_channel, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        \"\"\"\n",
    "        :param input_x: a list size having the number of batch_size elements with the same length\n",
    "        :return: batch_size X num_aspects tensor\n",
    "        \"\"\"\n",
    "        x = Variable(input_x)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Conv & max pool\n",
    "        x = x.unsqueeze(1)  # dim: (batch_size, 1, max_seq_len, embedding_size)\n",
    "\n",
    "        # turns to be a list: [ti : i \\in kernel_sizes] where ti: tensor of dim([batch, num_kernels, max_seq_len-i+1])\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv]\n",
    "\n",
    "        # dim: [(batch_size, num_kernels), ...]*len(kernel_sizes)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        # Dropout & output\n",
    "        x = self.dropout(x)  # (batch_size,len(kernel_sizes)*num_kernels)\n",
    "        # logit = F.log_softmax(self.fc(x))  # (batch_size, num_aspects)\n",
    "        fc = self.fc(x)\n",
    "        sigmoid = self.sigmoid(fc)\n",
    "        return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormTrainer:\n",
    "    def __init__(self, model, train_loader, test_loader, batch_size=BATCH_SIZE):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        self.criterion = torch.nn.BCELoss()\n",
    "        if USE_GPU:\n",
    "            self.model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, label) in enumerate(tqdm(self.train_loader)):\n",
    "            self.optimizer.zero_grad()\n",
    "            vdata = Variable(data)\n",
    "            vlabel = Variable(label)\n",
    "            if USE_GPU:\n",
    "                vdata = vdata.cuda()\n",
    "                vlabel = vlabel.cuda()\n",
    "            predict = self.model(vdata)\n",
    "            predict = torch.squeeze(predict)\n",
    "            loss = self.criterion(predict, vlabel)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.step()\n",
    "        print('Average loss: {:.4f}'.format(train_loss / len(self.train_loader)))\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        predict_numpy = np.zeros(0)\n",
    "        label_numpy = np.zeros(0)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, label) in enumerate(tqdm(self.test_loader)):\n",
    "                vdata = Variable(data)\n",
    "                vlabel = Variable(label)\n",
    "                if USE_GPU:\n",
    "                    vdata = vdata.cuda()\n",
    "                    vlabel = vlabel.cuda()\n",
    "                predict = self.model(vdata)\n",
    "                predict = torch.squeeze(predict)\n",
    "                predict_numpy = np.append(predict_numpy, predict.cpu().numpy())\n",
    "                label_numpy = np.append(label_numpy, vlabel.cpu().numpy())\n",
    "                loss = self.criterion(predict, vlabel)\n",
    "                test_loss += loss.item()\n",
    "        print(\"Total loss: {:.4f}\".format(test_loss))\n",
    "        print(\"F1 score is: {: .4f}\".format(NormTrainer.get_f1_score_by_predict_sigmoid(predict_numpy, label_numpy)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_f1_score_by_predict_sigmoid(predict, true_label):\n",
    "        predict = np.round(predict)\n",
    "        true_label = np.round(true_label)\n",
    "        return f1_score(predict, true_label)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 25\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCNN(pretrained_embeddings)\n",
    "if USE_GPU:\n",
    "    model = model.double().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4847 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================  EPOCH 0  ===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1303/4847 [01:21<03:34, 16.53it/s]"
     ]
    }
   ],
   "source": [
    "trainer = NormTrainer(model=model, train_loader=train_iter, test_loader=test_iter)\n",
    "for epoch in range(EPOCH):\n",
    "        print(\"===============================  EPOCH {:d}  ===============================\".format(epoch))\n",
    "        trainer.train()\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"===============================  Test  ===============================\")\n",
    "            trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/test.csv')\n",
    "submission_df = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text_numpy):\n",
    "    preprocess_text_result = []\n",
    "    for text in text_numpy:\n",
    "        preprocess_text = TEXT.preprocess(text)\n",
    "        preprocess_text_result.append(preprocess_text)\n",
    "    processed_text = TEXT.process(preprocess_text_result)\n",
    "    TEST_BATCH_SIZE = 256\n",
    "    idx = 0\n",
    "    predict_vector = np.zeros(0)\n",
    "    with torch.no_grad():\n",
    "        while idx < len(processed_text):\n",
    "            batch_data = processed_text[idx: idx+TEST_BATCH_SIZE]\n",
    "            if USE_GPU:\n",
    "                batch_data = batch_data.cuda()\n",
    "            predicts = model(batch_data)\n",
    "            predicts = torch.squeeze(predicts)\n",
    "            predicts_numpy = predicts.cpu().numpy()\n",
    "            predict_vector = np.append(predict_vector, predicts_numpy)\n",
    "            idx += TEST_BATCH_SIZE\n",
    "    return predict_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = predict(model, test_df.question_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.round(predict_result).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('../input/sample_submission.csv')\n",
    "submission_df['prediction'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
