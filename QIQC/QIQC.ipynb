{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "import nltk\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(predict, threshold=0.36):\n",
    "    return (predict >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhanceIter(object):\n",
    "    \"\"\" 数据加强类\n",
    "    input: 数据集 原生数据集比率 \n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, dataset, ratio=0.8, batch_size=256):\n",
    "        self.batch_size = batch_size\n",
    "        self.ratio = ratio\n",
    "        self.dataset = dataset\n",
    "        self.origin_loader = torchtext.data.Iterator(dataset=dataset, \n",
    "                                                     batch_size=math.ceil(ratio*batch_size), \n",
    "                                                     shuffle=True)\n",
    "        self.postive_idxs = self._get_postive_idxs()\n",
    "        self.TEXT = self.dataset.fields['question_text']\n",
    "        self.iter_origin = iter(self.origin_loader)\n",
    "        self.postive_count = self.batch_size - math.ceil(self.ratio*self.batch_size)\n",
    "        \n",
    "    def _get_postive_idxs(self):\n",
    "        postive_list = []\n",
    "        for idx, example in enumerate(self.dataset):\n",
    "            if (example.target == str(1)):\n",
    "                postive_list.append(idx)\n",
    "        return np.array(postive_list, dtype=np.int64)\n",
    "    \n",
    "    def init_epoch(self):\n",
    "        self.origin_loader = torchtext.data.Iterator(dataset=self.dataset, \n",
    "                                             batch_size=math.ceil(self.ratio*self.batch_size), \n",
    "                                             shuffle=True)\n",
    "        self.iter_origin = iter(self.origin_loader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.floor(len(self.dataset) / self.ratio / self.batch_size)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            self.init_epoch()\n",
    "            for _ in range(len(self)-1):\n",
    "                enhace_data = self.get_batch_enhace_data()\n",
    "                enhace_label = torch.ones(self.postive_count)\n",
    "                origin_data, origin_label = next(self.iter_origin)\n",
    "                batch_data = torch.cat((enhace_data, origin_data), 0)\n",
    "                batch_label = torch.cat((enhace_label, origin_label), 0)\n",
    "                idx = torch.randperm(len(batch_label))\n",
    "                batch_data = batch_data[idx]\n",
    "                batch_label = batch_label[idx]\n",
    "                yield batch_data, batch_label\n",
    "            return\n",
    "\n",
    "    def get_batch_enhace_data(self):\n",
    "        idxs = self._get_postive_idx_in_batch()\n",
    "        enhance_texts = self._get_enhance_texts_with_idxs(idxs)\n",
    "        preprocess_texts = []\n",
    "        for text in enhance_texts:\n",
    "            preprocess_text = self.TEXT.preprocess(text)\n",
    "            preprocess_texts.append(preprocess_text)\n",
    "        processed_text = self.TEXT.process(preprocess_texts)\n",
    "        return processed_text\n",
    "        \n",
    "    def _get_enhance_texts_with_idxs(self, idxs):\n",
    "        enhance_texts = []\n",
    "        for idx in idxs:\n",
    "            postive_text = self.dataset[idx].question_text\n",
    "            enhance_text = self._enhance_text(postive_text)\n",
    "            enhance_texts.append(enhance_text)\n",
    "        return enhance_texts\n",
    "    \n",
    "    def _get_postive_idx_in_batch(self):\n",
    "        idxs = np.random.choice(self.postive_idxs, self.postive_count)\n",
    "        return idxs\n",
    "    \n",
    "    def _enhance_text(self, text):\n",
    "        \"\"\"\n",
    "        对文本进行增强\n",
    "        \n",
    "        目前的方法是打乱顺序\n",
    "        \n",
    "        input: text 为一个文本 eg. ['hello', 'world']\n",
    "        output: 增强后的文本 eg. ['world', 'hello']\n",
    "        \n",
    "        \"\"\"\n",
    "        return random.sample(text, k=len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = torchtext.vocab.Vectors(\"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIX_LENGTH = 70\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "TEXT = torchtext.data.Field(tokenize=torchtext.data.get_tokenizer('toktok'), init_token='<SOS>', \n",
    "                            eos_token='<EOS>',lower=True, fix_length=FIX_LENGTH, stop_words=None, \n",
    "                            batch_first=True)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True, batch_first=True, \n",
    "                             dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchtext.data.TabularDataset(\n",
    "    path='../input/train.csv', format = 'csv', \n",
    "    fields=[('qid', None),\n",
    "            ('question_text', TEXT), \n",
    "            ('target', LABEL)], \n",
    "    skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_data.split(split_ratio=0.95, strata_field='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors = glove_vectors, max_size=40000)\n",
    "vocab = TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 40004\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained embeddings shape: torch.Size([40004, 300])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = vocab.vectors\n",
    "print(f\"Pretrained embeddings shape: {pretrained_embeddings.shape}\")\n",
    "EMBEDDING_LENGTH = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_sizes = [1,2,3,5]\n",
    "# filter_sizes_sum = sum(filter_sizes)\n",
    "# num_filters = 42\n",
    "# CNN_DROPOUT = 0.1\n",
    "# class TextCNN(nn.Module):\n",
    "#     def __init__(self, pretrained_weight):\n",
    "#         super(TextCNN, self).__init__()\n",
    "#         self.embedding = nn.Embedding.from_pretrained(pretrained_weight, freeze=False)\n",
    "#         self.conv0 = nn.Sequential(\n",
    "#             nn.Conv2d(1, num_filters, kernel_size=(filter_sizes[0], EMBEDDING_LENGTH)),\n",
    "#             nn.Tanh())\n",
    "#         self.conv0.apply(self.init_weight)\n",
    "#         self.pool0 = nn.MaxPool2d((FIX_LENGTH-filter_sizes[0]+1, 1), stride=1)\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, num_filters, kernel_size=(filter_sizes[0], EMBEDDING_LENGTH)),\n",
    "#             nn.Tanh())\n",
    "#         self.conv1.apply(self.init_weight)\n",
    "#         self.pool1 = nn.MaxPool2d((FIX_LENGTH-filter_sizes[1]+1, 1), stride=1)\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(1, num_filters, kernel_size=(filter_sizes[0], EMBEDDING_LENGTH)),\n",
    "#             nn.Tanh())\n",
    "#         self.conv2.apply(self.init_weight)\n",
    "#         self.pool2 = nn.MaxPool2d((FIX_LENGTH-filter_sizes[2]+1, 1), stride=1)\n",
    "#         self.conv3 = nn.Sequential(\n",
    "#             nn.Conv2d(1, num_filters, kernel_size=(filter_sizes[0], EMBEDDING_LENGTH)),\n",
    "#             nn.Tanh())\n",
    "#         self.conv3.apply(self.init_weight)\n",
    "#         self.pool3 = nn.MaxPool2d((FIX_LENGTH-filter_sizes[3]+1, 1), stride=1)\n",
    "#         self.dropout = nn.Dropout(CNN_DROPOUT)\n",
    "#         self.fc = nn.Linear(filter_sizes_sum * 42, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x_input):\n",
    "#         x = self.embedding(x_input)\n",
    "#         x = x.unsqueeze(1)\n",
    "#         conv0 = self.conv0(x)\n",
    "#         pool0 = self.pool0(conv0)\n",
    "#         conv1 = self.conv1(x)\n",
    "#         pool1 = self.pool1(conv1)\n",
    "#         conv2 = self.conv2(x)\n",
    "#         pool2 = self.pool2(conv2)\n",
    "#         conv3 = self.conv3(x)\n",
    "#         pool3 = self.pool3(conv3)\n",
    "#         z = torch.cat((pool0, pool1, pool2, pool3), 2)\n",
    "#         z = z.view(z.size(0), -1)\n",
    "#         z = self.dropout(z)\n",
    "#         x = self.fc(z)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x\n",
    "\n",
    "#     def init_weight(self, layer):\n",
    "#         if (type(layer) == nn.Conv2d):\n",
    "#             nn.init.kaiming_normal_(layer.weight, mode='fan_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [1,2,3,6]\n",
    "filter_sizes_len = len(filter_sizes)\n",
    "num_filters = 36\n",
    "CNN_DROPOUT = 0.1\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, pretrained_weight):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_weight, freeze=False)\n",
    "        self.conv0 = nn.Sequential(\n",
    "            nn.Conv1d(EMBEDDING_LENGTH, num_filters, kernel_size=(filter_sizes[0])),\n",
    "            nn.ELU())\n",
    "        self.conv0.apply(self.init_weight)\n",
    "        self.pool0 = nn.MaxPool1d((FIX_LENGTH-filter_sizes[0]+1))\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(EMBEDDING_LENGTH, num_filters, kernel_size=(filter_sizes[1])),\n",
    "            nn.ELU())\n",
    "        self.conv1.apply(self.init_weight)\n",
    "        self.pool1 = nn.MaxPool1d((FIX_LENGTH-filter_sizes[1]+1))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(EMBEDDING_LENGTH, num_filters, kernel_size=(filter_sizes[2])),\n",
    "            nn.ELU())\n",
    "        self.conv2.apply(self.init_weight)\n",
    "        self.pool2 = nn.MaxPool1d((FIX_LENGTH-filter_sizes[2]+1))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(EMBEDDING_LENGTH, num_filters, kernel_size=(filter_sizes[3])),\n",
    "            nn.ELU())\n",
    "        self.conv3.apply(self.init_weight)\n",
    "        self.pool3 = nn.MaxPool1d((FIX_LENGTH-filter_sizes[3]+1))\n",
    "        self.batch_norm = nn.BatchNorm1d(filter_sizes_len * num_filters)\n",
    "        self.dropout = nn.Dropout(CNN_DROPOUT)\n",
    "        self.fc = nn.Linear(filter_sizes_len * num_filters, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        x = self.embedding(x_input)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        conv0 = self.conv0(x)\n",
    "        pool0 = self.pool0(conv0)\n",
    "        conv1 = self.conv1(x)\n",
    "        pool1 = self.pool1(conv1)\n",
    "        conv2 = self.conv2(x)\n",
    "        pool2 = self.pool2(conv2)\n",
    "        conv3 = self.conv3(x)\n",
    "        pool3 = self.pool3(conv3)\n",
    "        z = torch.cat((pool0, pool1, pool2, pool3), 1)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.dropout(z)\n",
    "#         z = self.batch_norm(z)\n",
    "        x = self.fc(z)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def init_weight(self, layer):\n",
    "        if (type(layer) == nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(layer.weight, mode='fan_in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormTrainer:\n",
    "    def __init__(self, model, train_loader, test_loader, get_label_f, batch_size=BATCH_SIZE):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        self.criterion = torch.nn.BCELoss()\n",
    "        if USE_GPU:\n",
    "            self.model.cuda()\n",
    "        self.get_label_f = get_label_f\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, label) in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            vdata = Variable(data)\n",
    "            vlabel = Variable(label)\n",
    "            if USE_GPU:\n",
    "                vdata = vdata.cuda()\n",
    "                vlabel = vlabel.cuda()\n",
    "            predict = self.model(vdata)\n",
    "            predict = torch.squeeze(predict)\n",
    "            loss = self.criterion(predict, vlabel)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.step()\n",
    "        print('Average loss: {:.4f}'.format(train_loss / len(self.train_loader)))\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        predict_numpy = np.zeros(0)\n",
    "        label_numpy = np.zeros(0)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, label) in enumerate(self.test_loader):\n",
    "                vdata = torch.LongTensor(data)\n",
    "                vlabel = torch.Tensor(label)\n",
    "                if USE_GPU:\n",
    "                    vdata = vdata.cuda()\n",
    "                    vlabel = vlabel.cuda()\n",
    "                predict = self.model(vdata)\n",
    "                predict = torch.squeeze(predict)\n",
    "                predict_numpy = np.append(predict_numpy, predict.cpu().numpy())\n",
    "                label_numpy = np.append(label_numpy, vlabel.cpu().numpy())\n",
    "                loss = self.criterion(predict, vlabel)\n",
    "                test_loss += loss.item()\n",
    "        print(\"Total loss: {:.4f}\".format(test_loss))\n",
    "        print(\"Threshold F1 score is: {: .4f}\".format(self.get_f1_score_by_predict_sigmoid(predict_numpy, label_numpy)))\n",
    "        print(\"Standard F1 score is: {: .4f}\".format(self.get_f1_score_by_predict_sigmoid(predict_numpy, label_numpy, 0.5)))\n",
    "    \n",
    "    def get_f1_score_by_predict_sigmoid(self, predict, true_label, threshold=None):\n",
    "        if threshold is not None:\n",
    "            predict = self.get_label_f(predict, threshold)\n",
    "        else:\n",
    "            predict = self.get_label_f(predict)\n",
    "        true_label = true_label.astype(np.int32)\n",
    "        return f1_score(predict, true_label)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 2\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter = torchtext.data.Iterator(dataset=train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_iter = EnhanceIter(dataset=train, batch_size=BATCH_SIZE, ratio=0.8)\n",
    "test_iter = torchtext.data.Iterator(dataset=test, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCNN(pretrained_embeddings)\n",
    "if USE_GPU:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================  EPOCH 0  ===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab403/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: generator 'EnhanceIter.__iter__' raised StopIteration\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.0984\n",
      "===============================  Test  ===============================\n",
      "Total loss: 27.4417\n",
      "Threshold F1 score is:  0.6504\n",
      "Standard F1 score is:  0.6364\n",
      "===============================  EPOCH 1  ===============================\n",
      "Average loss: 0.0788\n",
      "===============================  Test  ===============================\n",
      "Total loss: 27.4590\n",
      "Threshold F1 score is:  0.6538\n",
      "Standard F1 score is:  0.6341\n"
     ]
    }
   ],
   "source": [
    "trainer = NormTrainer(model=model, train_loader=train_iter, test_loader=test_iter, get_label_f=get_label)\n",
    "for epoch in range(EPOCH):\n",
    "        print(\"===============================  EPOCH {:d}  ===============================\".format(epoch))\n",
    "        trainer.train()\n",
    "        print(\"===============================  Test  ===============================\")\n",
    "        trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/test.csv')\n",
    "submission_df = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text_numpy):\n",
    "    preprocess_text_result = []\n",
    "    for text in text_numpy:\n",
    "        preprocess_text = TEXT.preprocess(text)\n",
    "        preprocess_text_result.append(preprocess_text)\n",
    "    processed_text = TEXT.process(preprocess_text_result)\n",
    "    TEST_BATCH_SIZE = 256\n",
    "    idx = 0\n",
    "    predict_vector = np.zeros(0)\n",
    "    with torch.no_grad():\n",
    "        while idx < len(processed_text):\n",
    "            batch_data = processed_text[idx: idx+TEST_BATCH_SIZE]\n",
    "            if USE_GPU:\n",
    "                batch_data = batch_data.cuda()\n",
    "            predicts = model(batch_data)\n",
    "            predicts = torch.squeeze(predicts)\n",
    "            predicts_numpy = predicts.cpu().numpy()\n",
    "            predict_vector = np.append(predict_vector, predicts_numpy)\n",
    "            idx += TEST_BATCH_SIZE\n",
    "    return predict_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = predict(model, test_df.question_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_label(predict_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('../input/sample_submission.csv')\n",
    "submission_df['prediction'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '['\n",
    "for i in result:\n",
    "    s += f\"{i},\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
