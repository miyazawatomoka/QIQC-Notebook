{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "import nltk\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(predict, threshold=0.23):\n",
    "    return (predict >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = torchtext.vocab.Vectors(\"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIX_LENGTH = 70\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "TEXT = torchtext.data.Field(tokenize=torchtext.data.get_tokenizer('toktok'), init_token='<SOS>', \n",
    "                            eos_token='<EOS>',lower=True, fix_length=FIX_LENGTH, stop_words=None, \n",
    "                            batch_first=True)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True, batch_first=True, \n",
    "                             dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchtext.data.TabularDataset(\n",
    "    path='../input/train.csv', format = 'csv', \n",
    "    fields=[('qid', None),\n",
    "            ('question_text', TEXT), \n",
    "            ('target', LABEL)], \n",
    "    skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_data.split(split_ratio=0.95, strata_field='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_iter = torchtext.data.Iterator(dataset=train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_iter = torchtext.data.Iterator(dataset=test, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors = glove_vectors, max_size=40000)\n",
    "vocab = TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 40004\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained embeddings shape: torch.Size([40004, 300])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = vocab.vectors\n",
    "print(f\"Pretrained embeddings shape: {pretrained_embeddings.shape}\")\n",
    "EMBEDDING_LENGTH = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_sizes = [1,2,3,5]\n",
    "# filter_sizes_sum = sum(filter_sizes)\n",
    "# num_filters = 42\n",
    "# CNN_DROPOUT = 0.1\n",
    "# class TextCNN(nn.Module):\n",
    "#     def __init__(self, pretrained_weight):\n",
    "#         super(TextCNN, self).__init__()\n",
    "#         self.embedding = nn.Embedding.from_pretrained(pretrained_weight, freeze=False)\n",
    "#         self.conv0 = nn.Sequential(\n",
    "#             nn.Conv2d(1, num_filters, kernel_size=(filter_sizes[0], EMBEDDING_LENGTH)),\n",
    "#             nn.Tanh())\n",
    "#         self.conv0.apply(self.init_weight)\n",
    "#         self.pool0 = nn.MaxPool2d((FIX_LENGTH-filter_sizes[0]+1, 1), stride=1)\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, num_filters, kernel_size=(filter_sizes[0], EMBEDDING_LENGTH)),\n",
    "#             nn.Tanh())\n",
    "#         self.conv1.apply(self.init_weight)\n",
    "#         self.pool1 = nn.MaxPool2d((FIX_LENGTH-filter_sizes[1]+1, 1), stride=1)\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(1, num_filters, kernel_size=(filter_sizes[0], EMBEDDING_LENGTH)),\n",
    "#             nn.Tanh())\n",
    "#         self.conv2.apply(self.init_weight)\n",
    "#         self.pool2 = nn.MaxPool2d((FIX_LENGTH-filter_sizes[2]+1, 1), stride=1)\n",
    "#         self.conv3 = nn.Sequential(\n",
    "#             nn.Conv2d(1, num_filters, kernel_size=(filter_sizes[0], EMBEDDING_LENGTH)),\n",
    "#             nn.Tanh())\n",
    "#         self.conv3.apply(self.init_weight)\n",
    "#         self.pool3 = nn.MaxPool2d((FIX_LENGTH-filter_sizes[3]+1, 1), stride=1)\n",
    "#         self.dropout = nn.Dropout(CNN_DROPOUT)\n",
    "#         self.fc = nn.Linear(filter_sizes_sum * 42, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x_input):\n",
    "#         x = self.embedding(x_input)\n",
    "#         x = x.unsqueeze(1)\n",
    "#         conv0 = self.conv0(x)\n",
    "#         pool0 = self.pool0(conv0)\n",
    "#         conv1 = self.conv1(x)\n",
    "#         pool1 = self.pool1(conv1)\n",
    "#         conv2 = self.conv2(x)\n",
    "#         pool2 = self.pool2(conv2)\n",
    "#         conv3 = self.conv3(x)\n",
    "#         pool3 = self.pool3(conv3)\n",
    "#         z = torch.cat((pool0, pool1, pool2, pool3), 2)\n",
    "#         z = z.view(z.size(0), -1)\n",
    "#         z = self.dropout(z)\n",
    "#         x = self.fc(z)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x\n",
    "\n",
    "#     def init_weight(self, layer):\n",
    "#         if (type(layer) == nn.Conv2d):\n",
    "#             nn.init.kaiming_normal_(layer.weight, mode='fan_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [1,2,3,5]\n",
    "filter_sizes_len = len(filter_sizes)\n",
    "num_filters = 36\n",
    "CNN_DROPOUT = 0.1\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, pretrained_weight):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_weight, freeze=False)\n",
    "        self.conv0 = nn.Sequential(\n",
    "            nn.Conv1d(EMBEDDING_LENGTH, num_filters, kernel_size=(filter_sizes[0])),\n",
    "            nn.ELU())\n",
    "        self.conv0.apply(self.init_weight)\n",
    "        self.pool0 = nn.MaxPool1d((FIX_LENGTH-filter_sizes[0]+1))\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(EMBEDDING_LENGTH, num_filters, kernel_size=(filter_sizes[1])),\n",
    "            nn.ELU())\n",
    "        self.conv1.apply(self.init_weight)\n",
    "        self.pool1 = nn.MaxPool1d((FIX_LENGTH-filter_sizes[1]+1))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(EMBEDDING_LENGTH, num_filters, kernel_size=(filter_sizes[2])),\n",
    "            nn.ELU())\n",
    "        self.conv2.apply(self.init_weight)\n",
    "        self.pool2 = nn.MaxPool1d((FIX_LENGTH-filter_sizes[2]+1))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(EMBEDDING_LENGTH, num_filters, kernel_size=(filter_sizes[3])),\n",
    "            nn.ELU())\n",
    "        self.conv3.apply(self.init_weight)\n",
    "        self.pool3 = nn.MaxPool1d((FIX_LENGTH-filter_sizes[3]+1))\n",
    "        self.batch_norm = nn.BatchNorm1d(filter_sizes_len * num_filters)\n",
    "        self.dropout = nn.Dropout(CNN_DROPOUT)\n",
    "        self.fc = nn.Linear(filter_sizes_len * num_filters, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        x = self.embedding(x_input)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        conv0 = self.conv0(x)\n",
    "        pool0 = self.pool0(conv0)\n",
    "        conv1 = self.conv1(x)\n",
    "        pool1 = self.pool1(conv1)\n",
    "        conv2 = self.conv2(x)\n",
    "        pool2 = self.pool2(conv2)\n",
    "        conv3 = self.conv3(x)\n",
    "        pool3 = self.pool3(conv3)\n",
    "        z = torch.cat((pool0, pool1, pool2, pool3), 1)\n",
    "        z = z.view(z.size(0), -1)\n",
    "#         z = self.dropout(z)\n",
    "        z = self.batch_norm(z)\n",
    "        x = self.fc(z)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def init_weight(self, layer):\n",
    "        if (type(layer) == nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(layer.weight, mode='fan_in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormTrainer:\n",
    "    def __init__(self, model, train_loader, test_loader, get_label_f, batch_size=BATCH_SIZE):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        self.criterion = torch.nn.BCELoss()\n",
    "        if USE_GPU:\n",
    "            self.model.cuda()\n",
    "        self.get_label_f = get_label_f\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, label) in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            vdata = Variable(data)\n",
    "            vlabel = Variable(label)\n",
    "            if USE_GPU:\n",
    "                vdata = vdata.cuda()\n",
    "                vlabel = vlabel.cuda()\n",
    "            predict = self.model(vdata)\n",
    "            predict = torch.squeeze(predict)\n",
    "            loss = self.criterion(predict, vlabel)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.step()\n",
    "        print('Average loss: {:.4f}'.format(train_loss / len(self.train_loader)))\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        predict_numpy = np.zeros(0)\n",
    "        label_numpy = np.zeros(0)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, label) in enumerate(self.test_loader):\n",
    "                vdata = torch.LongTensor(data)\n",
    "                vlabel = torch.Tensor(label)\n",
    "                if USE_GPU:\n",
    "                    vdata = vdata.cuda()\n",
    "                    vlabel = vlabel.cuda()\n",
    "                predict = self.model(vdata)\n",
    "                predict = torch.squeeze(predict)\n",
    "                predict_numpy = np.append(predict_numpy, predict.cpu().numpy())\n",
    "                label_numpy = np.append(label_numpy, vlabel.cpu().numpy())\n",
    "                loss = self.criterion(predict, vlabel)\n",
    "                test_loss += loss.item()\n",
    "        print(\"Total loss: {:.4f}\".format(test_loss))\n",
    "        print(\"Threshold F1 score is: {: .4f}\".format(self.get_f1_score_by_predict_sigmoid(predict_numpy, label_numpy)))\n",
    "        print(\"Standard F1 score is: {: .4f}\".format(self.get_f1_score_by_predict_sigmoid(predict_numpy, label_numpy, 0.5)))\n",
    "    \n",
    "    def get_f1_score_by_predict_sigmoid(self, predict, true_label, threshold=None):\n",
    "        if threshold is not None:\n",
    "            predict = self.get_label_f(predict, threshold)\n",
    "        else:\n",
    "            predict = self.get_label_f(predict)\n",
    "        true_label = true_label.astype(np.int32)\n",
    "        return f1_score(predict, true_label)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 2\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCNN(pretrained_embeddings)\n",
    "if USE_GPU:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================  EPOCH 0  ===============================\n",
      "===============================  Test  ===============================\n",
      "Total loss: 33.5108\n",
      "Threshold F1 score is:  0.6304\n",
      "Standard F1 score is:  0.6194\n",
      "===============================  EPOCH 1  ===============================\n",
      "===============================  Test  ===============================\n",
      "Total loss: 33.3757\n",
      "Threshold F1 score is:  0.6304\n",
      "Standard F1 score is:  0.6194\n"
     ]
    }
   ],
   "source": [
    "trainer = NormTrainer(model=model, train_loader=train_iter, test_loader=test_iter, get_label_f=get_label)\n",
    "for epoch in range(EPOCH):\n",
    "        print(\"===============================  EPOCH {:d}  ===============================\".format(epoch))\n",
    "#         trainer.train()\n",
    "        print(\"===============================  Test  ===============================\")\n",
    "        trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/test.csv')\n",
    "submission_df = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text_numpy):\n",
    "    preprocess_text_result = []\n",
    "    for text in text_numpy:\n",
    "        preprocess_text = TEXT.preprocess(text)\n",
    "        preprocess_text_result.append(preprocess_text)\n",
    "    processed_text = TEXT.process(preprocess_text_result)\n",
    "    TEST_BATCH_SIZE = 256\n",
    "    idx = 0\n",
    "    predict_vector = np.zeros(0)\n",
    "    with torch.no_grad():\n",
    "        while idx < len(processed_text):\n",
    "            batch_data = processed_text[idx: idx+TEST_BATCH_SIZE]\n",
    "            if USE_GPU:\n",
    "                batch_data = batch_data.cuda()\n",
    "            predicts = model(batch_data)\n",
    "            predicts = torch.squeeze(predicts)\n",
    "            predicts_numpy = predicts.cpu().numpy()\n",
    "            predict_vector = np.append(predict_vector, predicts_numpy)\n",
    "            idx += TEST_BATCH_SIZE\n",
    "    return predict_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = predict(model, test_df.question_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_label(predict_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('../input/sample_submission.csv')\n",
    "submission_df['prediction'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
